{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This is tensorflow2 implement of The MomentumIterativeMethod attack.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils_attack import optimize_linear, compute_gradient\n",
    "from utils_attack import clip_eta\n",
    "\n",
    "\n",
    "\n",
    "def momentum_iterative_method(\n",
    "    model_fn,\n",
    "    x,\n",
    "    eps=0.3,\n",
    "    eps_iter=0.06,\n",
    "    nb_iter=10,\n",
    "    norm=np.inf,\n",
    "    clip_min=None,\n",
    "    clip_max=None,\n",
    "    y=None,\n",
    "    targeted=False,\n",
    "    decay_factor=1.0,\n",
    "    sanity_checks=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Tensorflow 2.0 implementation of Momentum Iterative Method (Dong et al. 2017).\n",
    "    Paper link: https://arxiv.org/pdf/1710.06081.pdf\n",
    "    :param model_fn: a callable that takes an input tensor and returns the model logits.\n",
    "    :param x: input tensor.\n",
    "    :param eps: (optional float) maximum distortion of adversarial example\n",
    "              compared to original input\n",
    "    :param eps_iter: (optional float) step size for each attack iteration\n",
    "    :param nb_iter: (optional int) Number of attack iterations.\n",
    "    :param norm: (optional) Order of the norm (mimics Numpy).\n",
    "              Possible values: np.inf, 1 or 2.\n",
    "    :param clip_min: (optional float) Minimum input component value\n",
    "    :param clip_max: (optional float) Maximum input component value\n",
    "    :param y: (optional) Tensor with true labels. If targeted is true, then provide the\n",
    "              target label. Otherwise, only provide this parameter if you'd like to use true\n",
    "              labels when crafting adversarial samples. Otherwise, model predictions are used\n",
    "              as labels to avoid the \"label leaking\" effect (explained in this paper:\n",
    "              https://arxiv.org/abs/1611.01236). Default is None.\n",
    "    :param targeted: (optional) bool. Is the attack targeted or untargeted?\n",
    "              Untargeted, the default, will try to make the label incorrect.\n",
    "              Targeted will instead try to move in the direction of being more like y.\n",
    "    :param decay_factor: (optional) Decay factor for the momentum term.\n",
    "    :param sanity_checks: bool, if True, include asserts (Turn them off to use less runtime /\n",
    "              memory or for unit tests that intentionally pass strange input)\n",
    "    :return: a tensor for the adversarial example\n",
    "    \"\"\"\n",
    "    # model forward prediction:\n",
    "    # model_normal = tf.keras.models.load_model(model_path)\n",
    "    # model_logits = tf.keras.models.load_model(model_logits_path)\n",
    "\n",
    "    if norm == 1:\n",
    "        raise NotImplementedError(\n",
    "            \"This attack hasn't been tested for norm=1.\"\n",
    "            \"It's not clear that FGM makes a good inner \"\n",
    "            \"loop step for iterative optimization since \"\n",
    "            \"it updates just one coordinate at a time.\"\n",
    "        )\n",
    "\n",
    "    # Check if order of the norm is acceptable given current implementation\n",
    "    if norm not in [np.inf, 1, 2]:\n",
    "        raise ValueError(\"Norm order must be either np.inf, 1, or 2.\")\n",
    "\n",
    "    asserts = []\n",
    "\n",
    "    # If a data range was specified, check that the input was in that range\n",
    "    if clip_min is not None:\n",
    "        asserts.append(tf.math.greater_equal(x, clip_min))\n",
    "\n",
    "    if clip_max is not None:\n",
    "        asserts.append(tf.math.less_equal(x, clip_max))\n",
    "\n",
    "    if y is None:\n",
    "        # Using model predictions as ground truth to avoid label leaking\n",
    "        y = tf.argmax(model_fn(x), 1)\n",
    "\n",
    "    # Initialize loop variables\n",
    "    momentum = tf.zeros_like(x)\n",
    "    adv_x = x\n",
    "\n",
    "    i = 0\n",
    "    while i < nb_iter:\n",
    "        # Define gradient of loss wrt input\n",
    "        grad = compute_gradient(model_fn, loss_fn, adv_x, y, targeted)\n",
    "\n",
    "        # Normalize current gradient and add it to the accumulated gradient\n",
    "        red_ind = list(range(1, len(grad.shape)))\n",
    "        avoid_zero_div = tf.cast(1e-12, grad.dtype)\n",
    "        grad = grad / tf.math.maximum(\n",
    "            avoid_zero_div,\n",
    "            tf.math.reduce_mean(tf.math.abs(grad), red_ind, keepdims=True),\n",
    "        )\n",
    "        momentum = decay_factor * momentum + grad\n",
    "\n",
    "        optimal_perturbation = optimize_linear(momentum, eps_iter, norm)\n",
    "        # Update and clip adversarial example in current iteration\n",
    "        adv_x = adv_x + optimal_perturbation\n",
    "        adv_x = x + clip_eta(adv_x - x, norm, eps)\n",
    "\n",
    "        if clip_min is not None and clip_max is not None:\n",
    "            adv_x = tf.clip_by_value(adv_x, clip_min, clip_max)\n",
    "        i += 1\n",
    "\n",
    "    if sanity_checks:\n",
    "        assert np.all(asserts)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    return adv_x\n",
    "\n",
    "\n",
    "def loss_fn(labels, logits):\n",
    "    \"\"\"\n",
    "    Added softmax cross entropy loss for MIM as in the original MI-FGSM paper.\n",
    "    \"\"\"\n",
    "\n",
    "    return tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset and the model\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_test = np.expand_dims(x_test, axis = -1).astype(np.float32) / 255 # randomize the input dataset\n",
    "\n",
    "# Load the pre-trained model\n",
    "mnist_model_logits = tf.keras.models.load_model(\"/ssd-sata1/mwt/def_project/DiffRobOT/MNIST/LeNet5_MNIST_logits.h5\")\n",
    "\n",
    "# Define a single MNIST image for testing\n",
    "test_image = x_test[0]  # Use the first test image\n",
    "test_label = y_test[0]  # Use the corresponding label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the MIM attack\n",
    "adv_image = momentum_iterative_method(\n",
    "    model_fn=mnist_model_logits,\n",
    "    x=tf.convert_to_tensor(np.expand_dims(test_image, axis=0)),  # Add batch dimension\n",
    "    eps=0.4,\n",
    "    eps_iter=0.08,\n",
    "    nb_iter=20,\n",
    "    norm=np.inf,\n",
    "    clip_min=0.0,\n",
    "    clip_max=1.0,\n",
    "    y=tf.convert_to_tensor([test_label]),  # True label, change if you want a targeted attack\n",
    "    targeted=False,\n",
    "    decay_factor=1.0,\n",
    "    sanity_checks=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Original Label: 7\n",
      "Adversarial Label: 5\n",
      "Total Perturbation (L2 norm): 8.054868698120117\n",
      "Total Iterations: 10\n",
      "Adversarial Image: [[0.4        0.         0.4        0.         0.         0.\n",
      "  0.4        0.         0.         0.39999998 0.32       0.\n",
      "  0.         0.         0.         0.         0.4        0.4\n",
      "  0.4        0.4        0.4        0.4        0.4        0.4\n",
      "  0.4        0.4        0.4        0.4       ]\n",
      " [0.4        0.         0.4        0.         0.         0.\n",
      "  0.         0.         0.         0.4        0.4        0.32\n",
      "  0.         0.         0.4        0.4        0.4        0.4\n",
      "  0.4        0.4        0.4        0.4        0.4        0.4\n",
      "  0.4        0.4        0.4        0.4       ]\n",
      " [0.         0.         0.         0.         0.4        0.\n",
      "  0.4        0.         0.4        0.4        0.4        0.4\n",
      "  0.         0.         0.         0.24       0.16       0.\n",
      "  0.4        0.39999998 0.4        0.4        0.4        0.\n",
      "  0.4        0.4        0.16       0.39999998]\n",
      " [0.08       0.         0.         0.         0.4        0.\n",
      "  0.4        0.         0.4        0.         0.         0.\n",
      "  0.         0.         0.         0.4        0.4        0.4\n",
      "  0.24       0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.16       0.         0.         0.         0.         0.\n",
      "  0.4        0.         0.4        0.4        0.4        0.4\n",
      "  0.4        0.4        0.4        0.4        0.4        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.4        0.4        0.         0.         0.4        0.24\n",
      "  0.4        0.4        0.4        0.4        0.4        0.4\n",
      "  0.4        0.         0.4        0.4        0.         0.\n",
      "  0.         0.         0.         0.         0.         0.08\n",
      "  0.16       0.4        0.4        0.4       ]\n",
      " [0.4        0.         0.         0.4        0.4        0.4\n",
      "  0.         0.4        0.4        0.4        0.4        0.4\n",
      "  0.4        0.         0.         0.4        0.         0.16\n",
      "  0.08       0.         0.         0.32       0.4        0.4\n",
      "  0.4        0.4        0.4        0.        ]\n",
      " [0.         0.4        0.         0.         0.         0.4\n",
      "  0.         0.3254902  0.22352943 0.19215688 0.         0.08\n",
      "  0.         0.08       0.         0.32       0.         0.\n",
      "  0.         0.         0.4        0.4        0.4        0.4\n",
      "  0.24       0.4        0.4        0.        ]\n",
      " [0.         0.         0.         0.         0.4        0.24\n",
      "  0.47058824 0.5960784  0.5960784  0.5960784  0.5960784  0.54509807\n",
      "  0.3764706  0.3764706  0.3764706  0.3764706  1.         1.\n",
      "  0.9364705  1.         1.         0.         0.24       0.\n",
      "  0.         0.4        0.4        0.08      ]\n",
      " [0.         0.         0.4        0.4        0.         0.\n",
      "  0.         0.20705883 0.08       0.04705882 0.2392157  0.49019608\n",
      "  0.5960784  0.48235294 0.5960784  1.         1.         1.\n",
      "  0.8180392  0.9960783  1.         0.14901963 0.         0.\n",
      "  0.         0.4        0.4        0.4       ]\n",
      " [0.         0.         0.4        0.08       0.         0.\n",
      "  0.         0.24       0.         0.4        0.4        0.46666667\n",
      "  0.08       0.45490196 0.6627451  0.6627451  0.         0.\n",
      "  0.         0.52549016 0.5960784  0.01568627 0.         0.\n",
      "  0.         0.4        0.4        0.4       ]\n",
      " [0.4        0.         0.08       0.         0.         0.\n",
      "  0.         0.         0.         0.4        0.4        0.4\n",
      "  0.4        0.4        0.4        0.4        0.         0.\n",
      "  0.         0.5921569  0.41960785 0.         0.         0.\n",
      "  0.         0.4        0.4        0.4       ]\n",
      " [0.4        0.         0.         0.4        0.08       0.16\n",
      "  0.         0.4        0.         0.4        0.4        0.\n",
      "  0.         0.4        0.4        0.4        0.4        0.\n",
      "  0.5137255  1.         0.2454903  0.         0.         0.\n",
      "  0.08       0.4        0.4        0.4       ]\n",
      " [0.4        0.4        0.4        0.4        0.4        0.\n",
      "  0.         0.         0.         0.4        0.4        0.4\n",
      "  0.4        0.24       0.4        0.4        0.24       0.90588236\n",
      "  1.         1.         0.17254911 0.         0.         0.\n",
      "  0.16       0.4        0.4        0.4       ]\n",
      " [0.4        0.4        0.4        0.4        0.4        0.\n",
      "  0.         0.         0.         0.         0.         0.4\n",
      "  0.4        0.4        0.4        0.4        0.6313726  1.\n",
      "  1.         0.6399999  0.4        0.4        0.         0.\n",
      "  0.4        0.4        0.4        0.32      ]\n",
      " [0.4        0.4        0.4        0.4        0.4        0.4\n",
      "  0.4        0.         0.         0.         0.         0.\n",
      "  0.         0.4        0.4        0.4        0.92156863 1.\n",
      "  1.         0.16       0.         0.         0.4        0.\n",
      "  0.4        0.4        0.16       0.        ]\n",
      " [0.         0.         0.32       0.4        0.4        0.4\n",
      "  0.4        0.4        0.4        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.40392157 0.572549\n",
      "  0.16       0.4        0.4        0.         0.         0.\n",
      "  0.4        0.4        0.4        0.4       ]\n",
      " [0.         0.         0.         0.4        0.4        0.4\n",
      "  0.4        0.4        0.4        0.         0.         0.4\n",
      "  0.         0.         0.         0.09411764 0.5960784  0.3137255\n",
      "  0.4        0.4        0.         0.         0.         0.\n",
      "  0.4        0.4        0.4        0.24      ]\n",
      " [0.         0.         0.         0.4        0.         0.39999998\n",
      "  0.4        0.4        0.4        0.4        0.4        0.4\n",
      "  0.4        0.4        0.69411767 1.         0.68000007 0.62352943\n",
      "  0.4        0.24       0.         0.         0.         0.4\n",
      "  0.08       0.4        0.4        0.4       ]\n",
      " [0.4        0.         0.         0.         0.         0.\n",
      "  0.32       0.4        0.4        0.4        0.4        0.4\n",
      "  0.4        0.4745098  1.         0.5960784  1.         0.4\n",
      "  0.39999998 0.39999998 0.         0.         0.         0.\n",
      "  0.4        0.4        0.4        0.        ]\n",
      " [0.24       0.16       0.08       0.         0.         0.\n",
      "  0.         0.         0.4        0.4        0.4        0.4\n",
      "  0.4117647  1.         1.         1.         0.5372549  0.4\n",
      "  0.08       0.         0.         0.         0.         0.24\n",
      "  0.4        0.4        0.4        0.        ]\n",
      " [0.4        0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.16       0.         0.\n",
      "  0.         0.5960784  0.5960784  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.4\n",
      "  0.4        0.4        0.         0.        ]\n",
      " [0.4        0.4        0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.47843137 0.5960784  0.05098039 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.4\n",
      "  0.4        0.4        0.4        0.        ]\n",
      " [0.4        0.4        0.4        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.08       0.12156865\n",
      "  0.5960784  0.5960784  0.         0.         0.24       0.4\n",
      "  0.4        0.         0.         0.32       0.24       0.4\n",
      "  0.08       0.4        0.4        0.4       ]\n",
      " [0.4        0.4        0.4        0.4        0.4        0.4\n",
      "  0.4        0.4        0.         0.4        0.6392157  1.\n",
      "  0.5960784  1.         0.6039216  0.4        0.4        0.4\n",
      "  0.4        0.         0.4        0.39999998 0.         0.\n",
      "  0.         0.4        0.4        0.4       ]\n",
      " [0.         0.4        0.4        0.4        0.4        0.4\n",
      "  0.4        0.         0.         0.         0.15450981 0.6760784\n",
      "  0.5960784  1.         0.39999998 0.4        0.4        0.4\n",
      "  0.         0.4        0.         0.32       0.         0.\n",
      "  0.         0.         0.4        0.4       ]\n",
      " [0.         0.         0.4        0.4        0.4        0.4\n",
      "  0.4        0.4        0.         0.         0.0745098  0.5960784\n",
      "  1.         0.         0.         0.4        0.4        0.4\n",
      "  0.08       0.4        0.4        0.4        0.4        0.24\n",
      "  0.         0.         0.4        0.4       ]\n",
      " [0.4        0.4        0.4        0.         0.         0.4\n",
      "  0.4        0.4        0.4        0.         0.4        0.4\n",
      "  0.4        0.         0.         0.08       0.         0.\n",
      "  0.         0.4        0.4        0.24       0.         0.4\n",
      "  0.         0.         0.4        0.4       ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd078209350>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Load the required model\n",
    "mnist_model = tf.keras.models.load_model(\"/ssd-sata1/mwt/def_project/DiffRobOT/MNIST/LeNet5_MNIST_normal.h5\")\n",
    "\n",
    "# Calculate total perturbation (in L2 norm)\n",
    "perturbation = np.linalg.norm(adv_image - test_image)\n",
    "total_perturbation = np.linalg.norm(perturbation)\n",
    "\n",
    "# Get original label\n",
    "original_label = np.argmax(mnist_model.predict(np.expand_dims(test_image, axis = 0)))\n",
    "# original_label = np.argmax(mnist_model_logits.predict(np.expand_dims(test_image, axis=0)))\n",
    "\n",
    "# Get adversarial label\n",
    "adv_label = np.argmax(mnist_model.predict(adv_image))\n",
    "# adv_label = np.argmax(mnist_model_logits.predict(adv_image))\n",
    "\n",
    "# Output results\n",
    "print(f\"Original Label: {original_label}\")\n",
    "print(f\"Adversarial Label: {adv_label}\")\n",
    "print(f\"Total Perturbation (L2 norm): {total_perturbation}\")\n",
    "print(f\"Total Iterations: {10}\")  # The number of iterations is specified by `nb_iter`\n",
    "\n",
    "# Convert the adversarial image back to numpy for visualization if needed\n",
    "adv_image_np = adv_image.numpy().squeeze()  # Remove batch and channel dimensions\n",
    "print(f\"Adversarial Image: {adv_image_np}\")\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title(\"no\")\n",
    "plt.imshow(adv_image_np, cmap=plt.cm.gray)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37-tf2-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
